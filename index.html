<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Mingjie Pan</title>
  <meta content="Mingjie Pan, https://pmj110119.github.io" name="keywords">
  <link rel="stylesheet" type="text/css" href="resources/css/mystyle.css">
  <link rel="stylesheet" type="text/css" href="resources/css/font.css">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164510176-1"></script>
  <script async defer src="https://buttons.github.io/buttons.js"></script>
  <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
          dataLayer.push(arguments);
      }
      gtag('js', new Date());
      gtag('config', 'UA-164510176-1');
  </script>
  <meta name="google-site-verification" content="rqWXbD70hMlVge3Lhc6VmIiXSzz1A1kLFWOCEGhUMWI" />
</head>

<body>
  <div class="section">
    <div class="paper">
        <img class="info" title="pmj" style="float: left; padding-left: .01em;" src="resources/images/dog.png">
        <!-- <img class="info" title="affiliation" style="float: right; padding-right: .01em;" src="resources/images/affiliation1.png"> -->
        <div class="info" style="padding-left: 14.5em; vertical-align: top;">
            <span style="line-height: 150%; font-size: 25pt;">Mingjie Pan (ÊΩòÈì≠Êù∞)</span><br>
            <strong>Affiliation</strong>: Peking University<br>
            <strong>Address</strong>: 5 Yiheyuan Road, Haidian District, Beijing, China<br>
            <strong>Email</strong>: pmj@stu.pku.edu.cn<br>
        </div>
        <div class="spanner"></div>
    </div>
</div>

<div class="section">
  <h2>About Me (<a href="https://github.com/pmj110119">[GitHub]</a>
      <a href="https://scholar.google.com/citations?user=QdUeY3IAAAAJ&hl=zh-CN&oi=ao">[Google Scholar]</a>
      <!-- <a href="resources/CV_Mingjie.pdf">[CV]</a> -->
      )
  </h2>
  <div class="paper">
      I'm currently a Ph.D candidate at the Peking University (PKU).<br><br>
      Previously, I received my B.E degree from UESTC and MS degree from PKU.
      <br>

      <!-- I work very close with my friends <a href="https://scholar.google.com/citations?user=42MVVPgAAAAJ">Dr. Enze
          Xie</a> and <a href="https://scholar.google.com/citations?user=oamjJdYAAAAJ">Prof. Xiang Li</a>. -->


      <br>
      My research interests focus on:
      <ul>
          <li>Embodied AI & Manipulation</li>
          <li>3D Vision</li>
      </ul>

  </div>
</div>


<div class="section">
  <h2 id="news">News</h2>
  <div class="paper">
      <ul>
        <li>
            2025/02: <strong><a href="https://omnimanip.github.io/">OmniManip</a></strong> and CheckManual are accepted by <b>CVPR 2025</b>, both as <alert>Highlight</alert> üéâ
         </li>
         <li>
            2025/02: We released  <strong><a href="https://huggingface.co/datasets/agibot-world/AgiBotDigitalWorld">AgibotDigitalWorld</a></strong>, a large manipulation dataset generated with high quality simulation.
         </li>
         <li>
            2024/12: LidarLLM is accepted by <b>AAAI 2025</b>.
         </li>
        <li>
            2024/01: <strong><a href="https://github.com/pmj110119/RenderOcc">RenderOcc</a></strong> is accepted by <b>ICRA 2024</b>!
         </li>
         <li>
            2023/12: One paper is accepted by <b>AAAI 2024</b>.
         </li>
         <li>
             2023/09: Ranked 2nd place in <b>ICCV 2023 Challenge</b> - Continual Test-time Adaptation for Semantic Segmentation.
         </li>
          <li>
              2023/06: One paper is selected by <b>MICCAI 2023</b>.
          </li>
          <li>
              2023/05: Ranked 3rd place in <b>CVPR 2023 Challenge</b> - <a href="https://opendrivelab.com/AD23Challenge.html">Vision-Centric 3D Occupancy Prediction</a>. 
          </li>
          <li>
              2023/03: One paper is selected by <b>CVPR 2023</b>.
          </li>
          <!-- <li>
              2021/09: Start my Master jounery in Peking University (PKU).
          </li> -->
          <li>
              2020/06: Win <b>Finalist Award</b> in <a href="https://www.comap.com/contests/mcm-icm">MCM/ICM</a> 2020 (Top 0.5%).
          </li>
          <li> 
              2020-06: Win <b>National First Prize</b> in <a href="https://www.robomaster.com/en-US">RoboMaster</a> 2020. 
          </li>
          <!-- <li> 
              2017-09: Start my Bachelor jounery in University of Electronic Science and technology (UESTC). 
          </li> -->
      </ul>

      <div class="spanner"></div>
  </div>
</div>

<!-- 
<div class="section">
  <h2 id="experience">Experience</h2>
  <div class="paper">
      <ul>
          <li>
              2023/04 - Present: Engineering Intern at Xiaomi Car, responsible for tasks such as Occupancy Prediction and Autolabeling in autonomous driving.
          </li>
          <li>
              2022/10 - 2023/04: Research Assistant at the Peking University (PKU), led by
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=voqw10cAAAAJ&view_op=list_works&sortby=pubdate">Prof. Shanghang Zhang</a>
          </li>
          <li>
              2022/06 - 2022/08: Research Intern at Huawei, led by
              <a href="https://scholar.google.com/citations?user=xDTcPZIAAAAJ&hl=zh-CN&oi=ao">Dr. Yihang Lou</a>
          </li>
          <li>
              2021/10 - 2022/05: Engineering Intern at Megvii, responsible for tasks such as 3D detection and depth estimation in autonomous driving.
          </li>
      </ul>

      <div class="spanner"></div>
  </div>
</div> -->

<div class="section">
  <h2>Selected Projects [<a href="https://scholar.google.com/citations?user=QdUeY3IAAAAJ&hl=zh-CN">Full List</a>]</h2>
  (* Equal contribution)


  <div class="paper">
    <img class="paper" src="resources/images/OmniManip.gif" title="OmniManip">
    <div><strong><a href="https://omnimanip.github.io/"> OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints</a></strong><br>
        <strong>Mingjie Pan*</strong>, Jiyao Zhang*, Tianshu Wu, Yinghao Zhao, Wenlong Gao, Hao Dong<br>
        <a href="https://arxiv.org/abs/2501.03841">Paper</a> |  <a href="https://github.com/pmj110119/OmniManip">Code</a> (coming soon) <br>
        <strong>CVPR 2025, <alert>Highlight</alert> üéâ</strong>
        <br>
    </div>
    <div class="spanner"></div>
    </div>



  <div class="paper"><img class="paper" src="resources/images/AgibotDigitalWorld.gif"
    title="AgibotDigitalWorld">
    <div><strong><a href="https://huggingface.co/datasets/agibot-world/AgiBotDigitalWorld">AgiBotDigitalWorld</a></strong><br>
        Jiyao Zhang, <strong>Mingjie Pan</strong>, Baifeng Xie, Yinghao Zhao, Wenlong Gao, Guangte Xiang, Jiawei Zhang, Dong Li, Zhijun Li, Sheng Zhang, Hongwei Fan, Chengyue Zhao, Shukai Yang, Maoqing Yao, Chuanzhe Suo, Hao Dong<br>
        <a href="https://huggingface.co/datasets/agibot-world/AgiBotDigitalWorld">Huggingface</a>, 2025
        <br>
        We will release the core data generation framework in months, plsease contact us if you want to join!

    </div>
    <div class="spanner"></div>
    </div>



  <div class="paper"><img class="paper" src="resources/images/RenderOcc.gif"
    title="RenderOcc">
    <div><strong><a href="https://github.com/pmj110119/RenderOcc">RenderOcc: Vision-Centric 3D Occupancy Prediction with 2D Rendering Supervision</a></strong><br>
        <strong>Mingjie Pan*</strong>, Jiaming Liu*, Renrui Zhang*, Peixiang Huang, Xiaoqi Li, Bing Wang, Hongwei Xie, Li Liu, Shanghang Zhang<br>
        <a href="https://arxiv.org/abs/2309.09502">Paper</a> | <a href="https://github.com/pmj110119/RenderOcc">Code</a> <br>
        <strong>ICRA 2024</strong>
        <br>
        <!-- <alert>We propose a universal 3D occupancy training paradigm that leverage 2D labels.</alert> -->

    </div>
    <div class="spanner"></div>
    </div>

    <div class="paper"><img class="paper" src="resources/images/LidarLLM.png"
        title="LidarLLM">
        <div><strong>LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding</strong><br>
            Senqiao Yang*, Jiaming Liu*, Ray Zhang*, <strong>Mingjie Pan*</strong>, Zoey Guo, Xiaoqi Li, Zehui Chen, Peng Gao, Yandong Guo, Shanghang Zhang<br>
            <a href="https://arxiv.org/abs/2312.14074">Paper</a>
            <br>
            <strong>AAAI 2025</strong>
            <!-- <alert>We propose a universal 3D occupancy training paradigm that leverage 2D labels.</alert> -->
    
        </div>
        <div class="spanner"></div>
        </div>
    

  <!-- <div class="paper"><img class="paper" src="resources/images/uniocc.png"
        title="uniocc">
    <div><strong>UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering</strong><br>
        <strong>Mingjie Pan</strong>, Li Liu,  Jiaming Liu, Peixiang Huang, Longlong Wang, Shanghang Zhang, Kuiyuan Yang<br>
        <font color="Red">3rd place</font> of CVPR 2023 Challenge - <a href="https://opendrivelab.com/AD23Challenge.html"> Vision-Centric 3D Occupancy Prediction </a><br> 
        <a href="https://arxiv.org/abs/2306.09117">Technical Report</a>, 2023<br> 
        
        <br>
    </div>
  <div class="spanner"></div>
</div> -->





  <div class="paper"><img class="paper" src="resources/images/DiffuseIR.png" title="DiffuseIR">
    <div><strong>DiffuseIR: Diffusion Models For Isotropic Reconstruction of 3D Microscopic Images</strong><br>
        <strong>Mingjie Pan*</strong>, Yulu Gan*, Fangxu Zhou, Jiaming Liu, Ying Zhang, Aimin Wang, Shanghang Zhang, Dawei Li<br>
        <a href="https://arxiv.org/abs/2306.12109">Paper</a><br>
        <strong>MICCAI 2023</strong>
        <br>
    </div>
    <div class="spanner"></div>
  </div>

  <!-- <div class="paper"><img class="paper" src="resources/images/sparse.png"
        title="sparse">
    <div><strong>Exploring sparse visual prompt for cross-domain semantic segmentation</strong><br>
        Senqiao Yang*, Jiarui Wu*, Jiaming Liu, Xiaoqi Li, Qizhe Zhang, <strong>Mingjie Pan</strong>, Shanghang Zhang<br>
        <a href="https://arxiv.org/abs/2303.09792">AAAI</a>, 2024<br>
        <br>
        <alert>Utilize visual prompts for domain adaptation in semantic segmentation tasks.</alert>

    </div>
    <div class="spanner"></div>
    </div> -->


  <div class="paper"><img class="paper" src="resources/images/cloud.png"
        title="cloud">
    <div><strong>Cloud-device collaborative adaptation to continual changing environments in the real-world</strong><br>
        Yulu Gan*, <strong>Mingjie Pan*</strong>, Rongyu Zhang, Zijian Ling, Lingran Zhao, Jiaming Liu, Shanghang Zhang<br>
        <a href="https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=QdUeY3IAAAAJ&citation_for_view=QdUeY3IAAAAJ:u5HHmVD_uO8C">Paper</a><br>
        <strong>CVPR 2023</strong>
        <br>
    </div>
  <div class="spanner"></div>
</div>



</div>



<style>
    img.robot {
      max-width: 25%;
      height: 150px;
    }
  </style>


<div class="section">
    <h2>Selected Awards</h2>
    <div class="paper">
        <ul>
  
          <li>3rd Prize of CVPR 2023 Challenge - Vision-Centric 3D Occupancy Prediction, May 2023 (Team leader)</li>
          <li>Silver medal (12th/1506) at Kaggle, Sartorius - Cell Instance Segmentation, Apr 2022 (Team leader)</li>
          <li>Finalist Award (Top 0.5% of 27149) at MCM/ICM, June 2020 (Team leader)</li>
          <li>1st Prize at RoboMaster, Mainland China Regional Competition, June 2020 (Team leader)</li>
          <li>2nd Prize at National Undergraduate Electronic Design Contest, Automatic quadcopter, Sept 2019 (Team leader)</li>
          <li>Outstanding Graduate Special Scholarship  (x2), 2018 - 2019</li>

        </ul>
    </div>
  </div>


  <div class="section">
    <h2>MISC</h2>
    <div class="paper">
        <ul>
  
            <li>Experience of robots from years ago.
                <br><br>
                <img class="robot" src="resources/images/RM/RM3.jpg" title="RoboMaster"><img class="robot" src="resources/images/RM/RM1.jpg" title="RoboMaster"><img class="robot" src="resources/images/RM/RM2.jpg" title="RoboMaster">
                <img class="robot" src="resources/images/RM/plane.jpg" title="RoboMaster"><img class="robot" src="resources/images/RM/robot.jpg" title="RoboMaster">



            </li>



        </ul>
    </div>
  </div>
  <br><br>


  
<div style='width:850px;height:300px;margin:0 auto'>
  <!--<a href="https://clustrmaps.com/site/1b7cl" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=wHZzsZcsBCB6HxOqmaDqDJX5oLT_yvrQ5HQx9agcvJo&cl=ffffff"></a>-->
  <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=j_dISv6vDKOSfzYlFCLhpH2_f0j868PaM_FrzQLA4po&cl=ffffff&w=a"></script>
</div>




</body>

</html>
